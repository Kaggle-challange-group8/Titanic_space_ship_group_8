from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
import numpy as np

# Define parameter grid
param_dist = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 10],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.3, 0.5],
    'reg_alpha': [0, 0.01, 0.1],
    'reg_lambda': [1, 1.5, 2.0]
}

# Initialize model
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Random search
random_search = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=param_dist,
    n_iter=30,
    scoring='accuracy',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Fit to training data
random_search.fit(X_train, y_train)

# Best parameters
print("Best Parameters:\n", random_search.best_params_)

# Evaluate on validation set
best_xgb = random_search.best_estimator_
y_pred_xgb = best_xgb.predict(X_val)

# Store metrics
metrics_dict['XGBoost Tuned'] = {
    'Accuracy': accuracy_score(y_val, y_pred_xgb),
    'Precision': precision_score(y_val, y_pred_xgb),
    'Recall': recall_score(y_val, y_pred_xgb),
    'F1': f1_score(y_val, y_pred_xgb)
}

# Plot confusion matrix and classification report
plot_confusion_matrix(y_val, y_pred_xgb, "XGBoost Tuned")
print_classification_report(y_val, y_pred_xgb, "XGBoost Tuned")
import shap

# Make sure X_train is a DataFrame if it's currently a NumPy array
X_train_df = pd.DataFrame(X_train, columns=X.columns)

# Create SHAP explainer for XGBoost
explainer_xgb = shap.TreeExplainer(best_xgb)

# Get SHAP values
shap_values_xgb = explainer_xgb.shap_values(X_train_df)

# Plot SHAP summary
shap.summary_plot(shap_values_xgb, X_train_df, show=True)
